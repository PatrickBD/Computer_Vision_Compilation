{
  "cells": [
    {
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "_uuid": "552049dd1f3849afe722fa888f803cb739bd38b4"
      },
      "cell_type": "code",
      "source": "from IPython.display import Image\nImage(\"../input/Dance_Robots_Comic.jpg\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ca221457a03fb13d3ed0fef66ddc3e1fb7360a58"
      },
      "cell_type": "markdown",
      "source": "(This is part 1 of 3 of my How to Teach an AI to Dance. I originally made 3 separate notebooks for this task before compiling them into one later. The complete assembled notebook of all 3 parts can be found here: https://www.kaggle.com/valkling/how-to-teach-an-ai-to-dance)\n\n# AI Dance Part 1: Video Preprocessing\n\nWe will be using the same video of training as in the youtube video. It is a >1 hour of go-go dancing female silhouettes. This is ideal as most other green screen dancing videos are too short, loops, and/or messy for easy preprocessing. While not greatly varied, this dancing is not looped and we do not have to cobble together multiple dance videos (which might also require different preprocessing steps for each).\n\nHere is the original video on youtube: https://www.youtube.com/watch?v=NdSqAAT28v0"
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\n\nimport skimage\nfrom PIL import Image\nfrom skimage.io import imread, imshow, imread_collection, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.util import crop, pad\nfrom skimage.morphology import label\nfrom skimage.color import rgb2gray, gray2rgb\n\nimport os\n\nimport zipfile\nz = zipfile.ZipFile(\"Dancer_Images.zip\", \"w\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "cap = cv2.VideoCapture('../input/Shadow Dancers 1 Hour.mp4')\nprint(cap.get(cv2.CAP_PROP_FPS))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f643678c8a2e54bfc3729504c9a364074d4351f2"
      },
      "cell_type": "code",
      "source": "%%time\n\ntry:\n    if not os.path.exists('data'):\n        os.makedirs('data')\nexcept OSError:\n    print ('Error: Creating directory of data')\n\ncurrentFrame = 0\ncount = 0\nTRAIN_SIZE = 27000\nFRAME_SKIP = 2\nIMG_WIDTH = 96\nIMG_HEIGHT = 64\nIMG_CHANNELS = 1\n\nvideo = cv2.VideoWriter('Simple_Shadow_Dancer_Video.avi',cv2.VideoWriter_fourcc(*\"MJPG\"), 30, (IMG_WIDTH, IMG_HEIGHT), False)\n\nwhile(count < TRAIN_SIZE):\n    try:\n        ret, frame = cap.read()\n\n        if currentFrame % FRAME_SKIP == 0:\n            count += 1\n            if count % int(TRAIN_SIZE/10) == 0:\n                print(str((count/TRAIN_SIZE)*100)+\"% done\")\n            # preprocess frames\n            img = frame\n            img = rgb2gray(img)\n            img = resize(img, (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), mode='constant', preserve_range=True)\n            img[img > 0.2] = 255\n            img[img <= 0.2] = 0\n            # save frame to zip and new video sample\n            name = './data/frame' + str(count) + '.jpg'\n            cv2.imwrite(name, img)\n            video.write(img.astype('uint8'))\n            z.write(name)\n            os.remove(name)\n    except:\n        print('Frame error')\n        break\n    currentFrame += 1\n\nprint(str(count)+\" Frames collected\")\ncap.release()\nz.close()\nvideo.release()\n\ncap.release()\nz.close()\nvideo.release()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "68144e108ce4633524f7059fc36acbb6e98deee7"
      },
      "cell_type": "markdown",
      "source": "## Part 1 Results\nThe dancer comes out clearly but a bit blocky. There is a bit of dirt in the frames but not too much and less than in the youtube video. The arms occasionally clip during quick motions, but that happens a lot in the original video as well just from the normal green screen clipping.\n\n### Possible Improvements\n- The frames of black and dirt as the video transitions to new dancers could be controlled and cleaned for.\n\n- Changing the binary threshold from 0.2. This can be a tradeoff between getting more of the dancer's pixels and picking up more dirt from the background.\n\n- To that effect, careful use of Image thresholding packages might work to cut out the dancer from the background too (but also might not work due to the constantly changing background in the video).\n\n- It is always an option to take larger and/or more frames of dancing for training, as long as we still got memory for it."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}