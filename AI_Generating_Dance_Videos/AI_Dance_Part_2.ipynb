{
  "cells": [
    {
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "_uuid": "3ae4b6ba1b582bb1864c500f72f0fed6e7b6c896"
      },
      "cell_type": "code",
      "source": "from IPython.display import Image\nImage(\"../input/Dance_Robots_Comic.jpg\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "db98bf05543586f9606ab0f0341ad401a30f1e4d"
      },
      "cell_type": "markdown",
      "source": "(This is part 2 of 3 of my How to Teach an AI to Dance. I originally made 3 separate notebooks for this task before compiling them into one later. The complete assembled notebook of all 3 parts can be found here: https://www.kaggle.com/valkling/how-to-teach-an-ai-to-dance)\n\n# AI Dance Part 2: Autoencoder Compression\n\nNow that we have the preprocessed frames from the shadow dancer video, we will still need to compress them much further to fit them into our RNN model. Among the many uses of autoencoders is making specialized compression models. In this section, we will train an autoencoder on our dance images and use it to compress the images into a much smaller numpy array, saving the model so that we can decode the images later."
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nimport sys\nimport random\nimport warnings\nfrom pylab import imshow, show, get_cmap\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nfrom itertools import chain\nimport skimage\nfrom PIL import Image\nfrom skimage.io import imread, imshow, imread_collection, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.util import crop, pad\nfrom skimage.morphology import label\n\nfrom keras.models import Model, load_model, Sequential\nfrom keras.layers import Input, Dense, UpSampling2D, Flatten, Reshape\nfrom keras.layers.core import Dropout, Lambda\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\nfrom keras import backend as K\nimport tensorflow as tf\n\nwarnings.filterwarnings('ignore', category=UserWarning, module='skimage')\nseed = 42\nrandom.seed = seed\nnp.random.seed = seed",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6652bfd0ffa4e195c3405555a4b5177c67d4dc53"
      },
      "cell_type": "markdown",
      "source": "## Read in Images"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "26602a219a4909a130d5fc01e7704e67138d8a45"
      },
      "cell_type": "code",
      "source": "# Set some parameters\nIMG_WIDTH = 96\nIMG_HEIGHT = 64\nIMG_CHANNELS = 1\nINPUT_SHAPE=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\nTRAIN_PATH = '../input/dancer_images/data/'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8f3d29f7d5609caff26b9def320e4a76611f5202"
      },
      "cell_type": "code",
      "source": "train_ids = next(os.walk(TRAIN_PATH))[2]\ntrain_ids[:10]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d03e8268642ce0ad749c52d5368b1a735fc94ee3"
      },
      "cell_type": "code",
      "source": "Image.open(TRAIN_PATH + 'frame5.jpg')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5ce8d3f5568e68e131a91fdb97a2f876fe0a8ed4"
      },
      "cell_type": "code",
      "source": "%%time\n\nX_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype='float32')\nfinal_train_ids = []\nmissing_count = 0\nprint('Getting train images ... ')\nsys.stdout.flush()\nfor n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):\n    path = TRAIN_PATH +'frame'+ str(n+1) + '.jpg'\n    try:\n        img = imread(path)\n        img = img.astype('float32') / 255.\n        img = resize(img, (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), mode='constant', preserve_range=True)\n        X_train[n-missing_count] = img\n    except:\n        print(\" Problem with: \"+path)\n        missing_count += 1\n        \nprint(\"Total missing: \"+ str(missing_count))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d7f44052eda740b3813309b65696a84581443ad1"
      },
      "cell_type": "code",
      "source": "for n in range(10,20):\n    imshow(X_train[n].reshape(64,96))\n    plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "eabcca55a39fca643df6ab142cd26ff5259ec384"
      },
      "cell_type": "markdown",
      "source": "## Create the Model\n\nIn addition to the Autoencoder model, we will also prepare an encoder and decoder for later. It is important to give the layers the same unique names and shapes in all 3 as we will be using the keras load_weights by_name option to copy our trained Autoencoder weights to each respective layer later."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "14b657873e4e9295e074e1d5e3bacc02d42d2bf5"
      },
      "cell_type": "code",
      "source": "def Encoder():\n    inp = Input(shape=INPUT_SHAPE)\n    x = Conv2D(128, (4, 4), activation='elu', padding='same',name='encode1')(inp)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same',name='encode2')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same',name='encode3')(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same',name='encode4')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same',name='encode5')(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same',name='encode6')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same',name='encode7')(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same',name='encode8')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(32, (3, 3), activation='elu', padding='same',name='encode9')(x)\n    x = Flatten()(x)\n    x = Dense(256, activation='elu',name='encode10')(x)\n    encoded = Dense(128, activation='sigmoid',name='encode11')(x)\n    return Model(inp, encoded)\n\nencoder = Encoder()\nencoder.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6ffd28bd9be5a7eb706c68927941898714fedf85"
      },
      "cell_type": "code",
      "source": "D_INPUT_SHAPE=[128]\ndef Decoder():\n    inp = Input(shape=D_INPUT_SHAPE, name='decoder')\n    x = Dense(256, activation='elu', name='decode1')(inp)\n    x = Dense(768, activation='elu', name='decode2')(x)\n    x = Reshape((4, 6, 32))(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same', name='decode3')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same', name='decode4')(x)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same', name='decode5')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same', name='decode6')(x)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(64, (2, 2), activation='elu', padding='same', name='decode7')(x)\n    x = Conv2D(128, (3, 3), activation='elu', padding='same', name='decode8')(x)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(64, (2, 2), activation='elu', padding='same', name='decode9')(x)\n    x = Conv2D(64, (4, 4), activation='elu', padding='same', name='decode10')(x)\n    x = Conv2D(128, (3, 3), activation='elu', padding='same', name='decode11')(x)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(64, (4, 4), activation='elu', padding='same', name='decode12')(x)\n    x = Conv2D(32, (3, 3), activation='elu', padding='same', name='decode13')(x)\n    x = Conv2D(16, (2, 2), activation='elu', padding='same', name='decode14')(x)\n    decoded = Conv2D(1, (2, 2), activation='sigmoid', padding='same', name='decode15')(x)\n    return Model(inp, decoded)\n\ndecoder = Decoder()\ndecoder.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "379a9d24e119055b9b746359f2c05a1dead14e1c"
      },
      "cell_type": "code",
      "source": "def Autoencoder():\n    inp = Input(shape=INPUT_SHAPE)\n    x = Conv2D(128, (4, 4), activation='elu', padding='same',name='encode1')(inp)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same',name='encode2')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same',name='encode3')(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same',name='encode4')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same',name='encode5')(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same',name='encode6')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same',name='encode7')(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same',name='encode8')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(32, (3, 3), activation='elu', padding='same',name='encode9')(x)\n    x = Flatten()(x)\n    x = Dense(256, activation='elu',name='encode10')(x)\n    encoded = Dense(128, activation='sigmoid',name='encode11')(x)\n    x = Dense(256, activation='elu', name='decode1')(encoded)\n    x = Dense(768, activation='elu', name='decode2')(x)\n    x = Reshape((4, 6, 32))(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same', name='decode3')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same', name='decode4')(x)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same', name='decode5')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same', name='decode6')(x)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(64, (2, 2), activation='elu', padding='same', name='decode7')(x)\n    x = Conv2D(128, (3, 3), activation='elu', padding='same', name='decode8')(x)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(64, (2, 2), activation='elu', padding='same', name='decode9')(x)\n    x = Conv2D(64, (4, 4), activation='elu', padding='same', name='decode10')(x)\n    x = Conv2D(128, (3, 3), activation='elu', padding='same', name='decode11')(x)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(64, (4, 4), activation='elu', padding='same', name='decode12')(x)\n    x = Conv2D(32, (3, 3), activation='elu', padding='same', name='decode13')(x)\n    x = Conv2D(16, (2, 2), activation='elu', padding='same', name='decode14')(x)\n    decoded = Conv2D(1, (2, 2), activation='sigmoid', padding='same', name='decode15')(x)\n    return Model(inp, decoded)\n\nmodel = Autoencoder()\nmodel.compile(optimizer='adam', loss='mean_squared_error')\nmodel.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "396c296aca3e414f269cb47359dfa65f008c979e"
      },
      "cell_type": "markdown",
      "source": "## Callbacks"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3e212da6f4b8651fde538066174d860aefe9c5b2"
      },
      "cell_type": "code",
      "source": "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n                                            patience=4, \n                                            verbose=1, \n                                            factor=0.5,\n                                            min_lr=0.00001)\nfilepath = \"Dancer_Auto_Model.hdf5\"\ncheckpoint = ModelCheckpoint(filepath,\n                             save_best_only=True,\n                             monitor='val_loss',\n                             mode='min')\n\nearly_stopping = EarlyStopping(monitor='val_loss',\n                              patience=8,\n                              verbose=1,\n                              mode='min',\n                              restore_best_weights=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "dfb7c25250819127589c862067c8a105631dacb1"
      },
      "cell_type": "markdown",
      "source": "### Custom Image Sample Callback\n\nHere is a custom callback I made named ImgSample. It tests the result of the autoencoder after every epoch by desplaying an sample image. The goal is to have the dancer come into focus as clearly as possible."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6a739a4660fe1e1b563890762cebd6c7dba4bad7"
      },
      "cell_type": "code",
      "source": "class ImgSample(Callback):\n\n    def __init__(self):\n       super(Callback, self).__init__() \n\n    def on_epoch_end(self, epoch, logs={}):\n        sample_img = X_train[50]\n        sample_img = sample_img.reshape(1, IMG_HEIGHT, IMG_WIDTH, 1)\n        sample_img = self.model.predict(sample_img)[0]\n        imshow(sample_img.reshape(64,96))\n        plt.show()\n\nimgsample = ImgSample()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ed9a69bc31ff13d0b5880b779fa4dd32b41fa98f"
      },
      "cell_type": "code",
      "source": "imshow(X_train[50].reshape(64,96))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a799ebe869de0abefc484deb5b5b80ddaf8ca916"
      },
      "cell_type": "markdown",
      "source": "## Train the Autoencoder"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2f2bbd8c98f30c143fba98c77502b482c4281475"
      },
      "cell_type": "code",
      "source": "%%time \nmodel.fit(X_train, X_train,\n          epochs=30, \n          batch_size=32,\n          verbose=2,\n          validation_split=0.05,\n        callbacks=[learning_rate_reduction, checkpoint, early_stopping, imgsample])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "731b1d623a4835b659ab77377cbc5c00c5d7a401"
      },
      "cell_type": "markdown",
      "source": "## Sample the Autoencoder Results\n\nThe reconstructions look pretty close to the originals, then the autoencoder works."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5595fbd538a2a827804e1f5657cbc8110e91bf6f"
      },
      "cell_type": "code",
      "source": "decoded_imgs = model.predict(X_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "46a6262281eabf011fece01610efc392f090e6ff"
      },
      "cell_type": "code",
      "source": "plt.figure(figsize=(20, 4))\nfor i in range(10):\n    # original\n    plt.subplot(2, 10, i + 1)\n    plt.imshow(X_train[i].reshape(IMG_HEIGHT, IMG_WIDTH))\n    plt.axis('off')\n \n    # reconstruction\n    plt.subplot(2, 10, i + 1 + 10)\n    plt.imshow(decoded_imgs[i].reshape(IMG_HEIGHT, IMG_WIDTH))\n    plt.axis('off')\n \nplt.tight_layout()\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "81d51b81e83d4837d4819c1d29023f0642678ed7"
      },
      "cell_type": "markdown",
      "source": "## Save Models and Create Encoded Dataset"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b12d1c18dc899f56cb6d09b57e861dff3a367c75"
      },
      "cell_type": "code",
      "source": "model.save_weights(\"Dancer_Auto_Weights.hdf5\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d7cca64fd5d22fee1efd763ef5eb29b5f055277c"
      },
      "cell_type": "code",
      "source": "encoder = Encoder()\ndecoder = Decoder()\n\nencoder.load_weights(\"Dancer_Auto_Weights.hdf5\", by_name=True)\ndecoder.load_weights(\"Dancer_Auto_Weights.hdf5\", by_name=True)\n\nmodel.save('Dancer_Auto_Model.hdf5') \ndecoder.save('Dancer_Decoder_Model.hdf5') \nencoder.save('Dancer_Encoder_Model.hdf5')\nmodel.save_weights(\"Dancer_Auto_Weights.hdf5\")\ndecoder.save_weights(\"Dancer_Decoder_Weights.hdf5\")\nencoder.save_weights(\"Dancer_Encoder_Weights.hdf5\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "14e70d09c6aa8c70983b5202e850adb1e2e5731f"
      },
      "cell_type": "code",
      "source": "Encoder_imgs = encoder.predict(X_train)\nEncoder_imgs.shape\nnp.save('Encoded_Dancer.npy',Encoder_imgs)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7a0495d2e51555d54ae2daf0add4560f943c841e"
      },
      "cell_type": "markdown",
      "source": "## Decode a Sample to Double Check Results\n\nIf the encoder and decoder models are working correctly, the dancer should appear like in the reconstruction of the autoencoder above."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f2b95cf0789b03698b2a56cc7291837fab7a15f5"
      },
      "cell_type": "code",
      "source": "decoded_imgs = decoder.predict(Encoder_imgs[0:20])\n\nplt.figure(figsize=(20, 4))\nfor i in range(10):\n    # reconstruction\n    plt.subplot(2, 10, i + 1 + 10)\n    plt.imshow(decoded_imgs[i].reshape(IMG_HEIGHT, IMG_WIDTH))\n    plt.axis('off')\n \nplt.tight_layout()\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ed8cee1c63cacb119b1c750c2c94bd4694829043"
      },
      "cell_type": "markdown",
      "source": "## Part 2 Results\nThe results are really good, there is only maybe a touch of blurriness around the hands after decoding. The image actually looks better than the binary image we started with. We can confidently proceed knowing that we can encode and decode the images without issue. I am quite happy with these results\n\n### Possible Improvements\n- The autoencoder works so well that we could do more without much issue, either compress further or use more detailed images.\n\n- The Autoencoder could be used to make a much much larger training set. Even if the uncompressed images get to big for the memory limit, it is possible to just train the autoencoder on a subset of the images then compress the whole set after. A 128 array is not that big, I don't foresee resource exhaustion errors being an major issue, even for much larger datasets."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}